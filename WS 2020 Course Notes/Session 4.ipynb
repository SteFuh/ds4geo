{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS4GEO_L4.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyMjnxK2s/FIFxE+734CWCJn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ds4geo/ds4geo/blob/master/WS%202020%20Course%20Notes/Session%204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgWSFU0RtK8A"
      },
      "source": [
        "# To Do\n",
        "\n",
        "* Create separate solutions notebook\n",
        "* Enable output cells\n",
        "* Final go-through and checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcdW54CLeSyH"
      },
      "source": [
        "# **Data Science for Geoscientists - Winter Semester 2020**\n",
        "# **Session 4 - Numpy etc. - 21st October 2020**\n",
        "In the previous sessions we learnt about python built-in objects useful for handling data. We also used the library Pandas for data handling, although didn't go into its details. The numerical basis of Pandas is a library called Numpy. This week we will introduce and learn the basics of Numpy including how to create, manipulate and index Numpy arrays.\n",
        "\n",
        "We will then apply that understanding in a geoscience related excercise: data reduction/calibration of raw Laser Ablation ICP Mass Spectrometer data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE7qVuiWebgJ"
      },
      "source": [
        "# Part 4.1 - Data Icebreaker - *Discussion*\n",
        "**Hypothesising about data**\n",
        "\n",
        "I will provide a sequence of 4 numbers which follow a rule.\n",
        "\n",
        "Participants have to figure out the rule.\n",
        "\n",
        "Participants can provide additional sequences, and I will answer yes or no as to whether they follow the rule.\n",
        "\n",
        "When participants think they know the rule, they write it down and give it to me.\n",
        "\n",
        "The first participant to correctly guess the rule wins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs8nRqBlhjtb"
      },
      "source": [
        "# Part 4.2 - Numpy Introduction - *Mini-lecture*\n",
        "Last week we used the popular python library Pandas, but didn't introduce it formally.\n",
        "This week we will also be using a popular libary called Numpy.\n",
        "Pandas is built upon Numpy, and they work well together.\n",
        "Pandas is good at data handling, manipulation and analysis, while Numpy is the basis of numerical operations and processing.\n",
        "See more here:\n",
        "* https://pandas.pydata.org/\n",
        "* https://numpy.org/\n",
        "\n",
        "We will use both Pandas and Numpy throughout the course. Together (along with matplotlib), they are the basis of Data Science in python.\n",
        "\n",
        "Numpy is based around multi-dimensional arrays (of data), and allows efficient indexing, operations and aggregation of said arrays.\n",
        "For those not familiar with multi-dimensional arrays (also called nd-arrays), imagine an excel spreadsheet as a 2 dimensional table/array with rows and columns, but that you can have as many dimensions as you like.\n",
        "\n",
        "As an example, in satellite remote sensing, it is typical to have a time-series of many multi-band (e.g. red, green, blue, infra-red) images. Therefore, you might have an array of 4 dimensions: [pixel rows, pixel columns, time, band]. So for each x-y pixel, at each point in time, you have a value for each band.\n",
        "\n",
        "In the following section, we will create arrays, learn how to do simple operations on them and perform basic aggregations. In the following section, we will explore Numpy's powerful indexing system.\n",
        "\n",
        "The website Datacamp.com provides an excellent Numpy \"cheat-sheet\". It is highly recommended to keep it handy when working with Numpy, and going through it in your own time.\n",
        "https://www.datacamp.com/community/blog/python-numpy-cheat-sheet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoBZl5Rjoiuq"
      },
      "source": [
        "# Part 4.3 - Numpy part 1 - *Walkthrough*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nxjB2a5l7SQ"
      },
      "source": [
        "## 4.3.1 - Creating Arrays\n",
        "\n",
        "Here we cover simple ways to create numpy arrays.\n",
        "We will cover loading and importing data, e.g. from pandas later.\n",
        "\n",
        "See also: https://scipy-cookbook.readthedocs.io/items/BuildingArrays.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eydcArwR-AGE"
      },
      "source": [
        "# Import numpy\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAPAdQKrr9V0"
      },
      "source": [
        "# The simplest way to create an array is from a list\n",
        "array = np.array([1,2,3])\n",
        "print(array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc8UDXZCwYV_"
      },
      "source": [
        "# Or with nested lists for multiple dimensions\n",
        "array_2d = np.array([[1,2,3],[4,5,6]])\n",
        "print(array_2d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5vUGGJs81eV"
      },
      "source": [
        "# numpy provides some functions to create arrays by shape:\n",
        "# make a 1d array of 5 zeros\n",
        "array_zeros = np.zeros(5) \n",
        "print(array_zeros)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pISRwawxwc1X"
      },
      "source": [
        "# Make a 2d array of 1s\n",
        "array_ones = np.ones((2,5))\n",
        "print(array_ones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMVftNwwwcit"
      },
      "source": [
        "# numpy arrays have an attribute shape, showing the size of an array in each dimension:\n",
        "print(\"array_zeros size:\", array_zeros.shape)\n",
        "print(\"array_ones size:\", array_ones.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl17kKwM9oAA"
      },
      "source": [
        "# Create an array of consecutive integers in a range using np.arange\n",
        "arange_1 = np.arange(15,25)\n",
        "print(arange_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEUWaTqVzC5k"
      },
      "source": [
        "# Use arange to create larger steps\n",
        "arange_2 = np.arange(15,25,2)\n",
        "print(arange_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2aRe7Y7zByL"
      },
      "source": [
        "# If one needs a standard python list in this style:\n",
        "print(range(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aW9N4XQ-Pfc"
      },
      "source": [
        "# Create array across range by number of intermediate steps, rather than the step itself\n",
        "linspace_1 = np.linspace(0,4,17)\n",
        "print(linspace_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbu5wLYikml7"
      },
      "source": [
        "# Arrays of random numbers can be produced with np.random.random_sample or np.random.standard_normal\n",
        "# random numbers in range 0 to 1\n",
        "uni_random = np.random.random_sample(10)\n",
        "print(uni_random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7J84_6KzQ6G"
      },
      "source": [
        "# Normal (gaussian) distributed random numbers\n",
        "# https://en.wikipedia.org/wiki/Normal_distribution\n",
        "norm_random = np.random.standard_normal(10)\n",
        "print(norm_random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GROW74LP24ap"
      },
      "source": [
        "# We can also create numpy arrays from Pandas dataframes or series:\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrmM-eVM2dhj"
      },
      "source": [
        "# Load Innsbruck weather data from csv file\n",
        "meteo = pd.read_csv(\"https://raw.githubusercontent.com/ds4geo/ds4geo/master/data/timeseries/meteo/Innsbruck_weather_2015-19.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y65PtSjt2-uU"
      },
      "source": [
        "# Create a numpy array from the whole dataframe\n",
        "npdataframe = meteo.to_numpy()\n",
        "print(npdataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGYyo1Lt3zFs"
      },
      "source": [
        "# Create numpy array from series (data frame column)\n",
        "npseries_temp = meteo[\"mean_temp (deg C)\"].to_numpy()\n",
        "print(npseries_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi7UEkuVl443"
      },
      "source": [
        "## 4.3.2 - Array Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QudTWd5mJ8R"
      },
      "source": [
        "# Python lets us do operations on integers and floats\n",
        "print(1+2)\n",
        "print(2*3)\n",
        "print(2.5*5)\n",
        "print(2**6)\n",
        "print(64/4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXu_TJ9wngK2"
      },
      "source": [
        "# But on lists, these operators do other things:\n",
        "print([1,2,3] + [4]) # List concatenation\n",
        "print([1,2,3] * 3) # List duplication\n",
        "# Operators like / and - do not work"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImMqH2EfyJU4"
      },
      "source": [
        "# Operators can be applied to numpy arrays in an intuitive way:\n",
        "# Operators between a numpy array and a single int or float apply the operation to all elements in the array:\n",
        "a = np.ones(5)\n",
        "\n",
        "print(\"a:\",a)\n",
        "print(\"a + 1:\",a + 1)\n",
        "print(\"a - 1:\",a - 1)\n",
        "print(\"a * 2:\",a * 2)\n",
        "print(\"a / 2:\",a / 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n82kfTjkzwRK"
      },
      "source": [
        "b = np.arange(5)\n",
        "print(\"b * 2:\", b * 2)\n",
        "print(\"b ^ 2:\", b ** 2)\n",
        "print(\"2 ^ b:\", 2 ** b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHDF6VEez_IN"
      },
      "source": [
        "# Operations between arrays of the same shape result are element-wise:\n",
        "print(\"b:\",b)\n",
        "print(\"b * b:\", b * b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoHf9v3o3M2T"
      },
      "source": [
        "## 4.3.3 - Aggregations\n",
        "We will look more into describing data using statistics in a future session. Here, we simply introduce how to calculate them on Numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua1XUbid3aau"
      },
      "source": [
        "# Calculate the mean\n",
        "# Create a large uniformly distributed random sample between 0 and 1\n",
        "large_uni_random = np.random.random_sample(10000)\n",
        "# Calculate and print various statistics\n",
        "print(\"mean:\",large_uni_random.mean())\n",
        "print(\"min:\",large_uni_random.min())\n",
        "print(\"max:\",large_uni_random.max())\n",
        "print(\"standard deviation:\",large_uni_random.std())\n",
        "\n",
        "# Note: Pandas uses a very similar object-orientated system: pd.DataFrame.mean?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BcgcyQk-7lt"
      },
      "source": [
        "# Above is the object orientated approach. An alternative is using numpy functions:\n",
        "print(\"mean:\",np.mean(large_uni_random))\n",
        "print(\"min:\",np.min(large_uni_random))\n",
        "print(\"max:\",np.max(large_uni_random))\n",
        "print(\"standard deviation:\",np.std(large_uni_random))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reNJ14Cy_3HE"
      },
      "source": [
        "\"\"\"\n",
        "If one's data contains NaNS (not a number - null values),\n",
        "one can use np.nanmean, np.nanmax, etc. to ignore the NaNs when calculating stats.\n",
        "\"\"\"\n",
        "# Create array with a NaN\n",
        "data_with_nans = np.array([1, 2, np.nan, 3])\n",
        "# Calculate mean with NaNs ignored\n",
        "print(np.nanmean(data_with_nans))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QGNl98ajWwh"
      },
      "source": [
        "\n",
        "# Part 4.4 - Numpy Excercise 1 - *Workshop*\n",
        "Follow the instructions below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8AAsIn80WeP"
      },
      "source": [
        "# Create the following sequences as numpy arrays:\n",
        "# 1. [3, 6, 9, 12, ...., 99]\n",
        "# 2. [15, 15, 15, 15, 15, 15]\n",
        "# 3. [0, 0.5, 1, 1.5, ...., 100]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRMHO66N3HNq"
      },
      "source": [
        "# Create the following arrays:\n",
        "# 4. 1d array of size 100 with random decimal numbers between 1 and 100\n",
        "# 5. 1d array of size 50 with random integers between 25 and 75\n",
        "# 6. 1d array of size 100 with normal (gaussian) distributed numbers with a mean of 5 and a standard deviation of 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj9WhCxd3HFC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlPg8rlxc7sr"
      },
      "source": [
        "# Challenges\n",
        "# 7. [0,1,0,2,0,4,0,8,0,16,0,32,0,64,.....65536]\n",
        "#   Note, this can be done in many different ways, including with other numpy functions\n",
        "#   and using numpy indexing, but it is possible to do with only the functions described above.\n",
        "# 8. An array representing the sum of rolling a pair of 6 sided dice 1000 times (if your game of monopoly overruns more than usual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkaFGg2jdxEU"
      },
      "source": [
        "## ANSWER (2 ** np.arange(-0.5,50.5,0.5)) * np.array([0,1]*51)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAgJFIO5r9o2"
      },
      "source": [
        "# Part 4.5 - Numpy part 2 - *Walkthrough*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVjj9ES3busy"
      },
      "source": [
        "##4.5.1 - Multi-dimensional arrays and Broadcasting\n",
        "So far we've looked at 1 dimensional arrays (ca. data with many rows but 1 column). Now we will look at multi-dimensional arrays and how we can operate on them using broadcasting (ca. data with rows and columns, and additional dimensions). We will look here at 2d arrays, but numpy allows arrays of any dimensionality.\n",
        "\n",
        "More info:\n",
        "* nd arrays: https://numpy.org/doc/stable/reference/arrays.ndarray.html\n",
        "* Broadcasting: https://numpy.org/doc/stable/user/basics.broadcasting.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jLSIBPHsXyU"
      },
      "source": [
        "# Create a multi-dimensional array using Numpy commands\n",
        "zeros_2d = np.zeros((5,9)) # Provide a list of dimension sizes to functions like np.zeros\n",
        "print(zeros_2d.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtePrXXc-ynQ"
      },
      "source": [
        "# Create a multi-dimensional array using stacking\n",
        "c1 = (np.random.random_sample(5)*2) +2 # 1d array of random numbers with mean of 2 and stdev of 2\n",
        "c2 = np.zeros(5) + 19 # 1d array of 19s\n",
        "c3 = np.full(5,np.nan) # 1d array of NaNs\n",
        "vertical_stack = np.vstack([c1, c2, c3]) # \"vertical\" stack\n",
        "horizontal_stack = np.hstack([c1, c2, c3]) # \"horizontal\" stack - in this case is concatenation\n",
        "\n",
        "print(\"vertical stack:\")\n",
        "print(vertical_stack)\n",
        "print(\"shape:\", vertical_stack.shape)\n",
        "\n",
        "print(\"horizontal stack:\")\n",
        "print(horizontal_stack)\n",
        "print(\"shape:\", horizontal_stack.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6gCuRKTAdkh"
      },
      "source": [
        "'''\n",
        "The vertical stack array produced what we wanted, but we intended c1, c2 and c3\n",
        "to be columns rather than rows (1st value in shape = nrows, 2nd = ncolumns).\n",
        "We can transpose the array to flip the rows and columns:\n",
        "'''\n",
        "# transpose vertical_stack\n",
        "vertical_transposed = vertical_stack.T\n",
        "print(\"vertical transposed:\")\n",
        "print(vertical_transposed)\n",
        "print(\"shape:\", vertical_transposed.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8-czK_o94Yd"
      },
      "source": [
        "'''\n",
        "Side note: aggregations and other numpy functions on multi dimensional arrays:\n",
        "there is an \"axis\" argument for many numpy functions which defines which dimension\n",
        "the operation is performed on.\n",
        "'''\n",
        "# nanmean to ignore NaN values\n",
        "print(\"without axis argument:\", np.nanmean(vertical_transposed)) # mean of all values in array\n",
        "print(\"axis=0 (columns):\", np.nanmean(vertical_transposed, axis=0)) # mean of columns\n",
        "print(\"axis=1 (rows):\", np.nanmean(vertical_transposed, axis=1)) # mean of rows\n",
        "# The last line gives a warning because it tries to calculate the mean of only NaNs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTfR5IpY5eb2"
      },
      "source": [
        "'''\n",
        "Broadcasting allows us to apply mathematical operations between arrays of different shapes\n",
        "where it is obvious what is intended.\n",
        "'''\n",
        "# Create a 2d array with differently distributed random normal distriubtions\n",
        "Sr = np.random.standard_normal(100) * 5 + 15\n",
        "Zn = np.random.standard_normal(100) * 0.2 + 0.5\n",
        "Fe = np.random.standard_normal(100) * 10 + 1000\n",
        "elements = np.vstack([Sr, Zn, Fe]).T # stack and transpose\n",
        "print(elements.shape) # 100 rows and 3 columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeI1lTZc6v9C"
      },
      "source": [
        "# Calculate the mean of each element\n",
        "ele_mean = np.mean(elements, axis=0)\n",
        "print(ele_mean)\n",
        "print(ele_mean.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8QettYz7fn4"
      },
      "source": [
        "'''\n",
        "The mean of each column is length 3 (the number of columns/elements)\n",
        "If we try to subtract the ele_mean of length 3 from tge element array (100, 3)\n",
        "it is obvious we mean to subtract the mean of each column from all values in each column:\n",
        "'''\n",
        "# Centre the data on 0 by subtracting the mean\n",
        "centered_elements = elements-ele_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY_HcUmG8ORp"
      },
      "source": [
        "# Standardize the data by dividing by the column standard deviation\n",
        "standardized_elements = centered_elements / np.std(centered_elements)\n",
        "\n",
        "# Now the mean of each element should be zero, and the standard deviation 1:\n",
        "print(\"mean:\", np.mean(standardized_elements))\n",
        "print(\"std:\", np.std(standardized_elements))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epksEZRxcAr_"
      },
      "source": [
        "## 4.5.2 - Logical operations and booleans\n",
        "We can compare Numpy arrays with comparison operators, resulting in boolean arrays with True or False.\n",
        "This can be done element wise with two arrays (corresponding elements are compared), or applied to a whole array if compared to e.g. an int or float value.\n",
        "Boolean arrays can be combined with numpy logical operator functions, e.g. np.logical_and, which operates elementwise on two boolean arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bK77j8nC_cd"
      },
      "source": [
        "# Create some arrays of ints between 0 and 9\n",
        "r1 = np.random.randint(0,10,10)\n",
        "r2 = np.random.randint(0,10,10)\n",
        "r3 = np.random.randint(0,10,10)\n",
        "\n",
        "print(r1)\n",
        "print(r2)\n",
        "print(r3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQlSE7u6ExNu"
      },
      "source": [
        "# Apply some comparison operations (>, <, =>, <=, ==)\n",
        "b1 = r1 > r2\n",
        "print(b1) # Return True where r1 is greater than r2 element-wise\n",
        "\n",
        "b2 = r3 > 5\n",
        "print (b2) # Return True when r3 is greater than 5\n",
        "\n",
        "b3 = np.logical_and(b1, b2) # element-wise logical and\n",
        "print(b3) # Return True when b1 and b2 are true\n",
        "# see also np.logical_or, _xor, _not, and np.all, np.any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexOOci1cG1M"
      },
      "source": [
        "## 4.5.3 - Numpy Indexing\n",
        "Last week we learnt how to index lists and dictionaries. Indexing in Numpy operates on a similar basis but with additional power and flexibility.\n",
        "\n",
        "More info:\n",
        "* https://scipy-cookbook.readthedocs.io/items/Indexing.html\n",
        "* https://numpy.org/doc/stable/user/basics.indexing.html#basics-indexing\n",
        "* https://numpy.org/doc/stable/reference/arrays.indexing.html#arrays-indexing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFTH-c4L-EMy"
      },
      "source": [
        "# Simple 1d indexing works like for lists\n",
        "arr = np.random.randint(0,100,15) # create an array of random ints\n",
        "print(arr)\n",
        "print(\"position 5:\", arr[5])\n",
        "print(\"positions 2 to 8:\", arr[2:8])\n",
        "print(\"positions every 2nd from 2 to 12:\", arr[2:12:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KpjD_yT-wHp"
      },
      "source": [
        "# If there are more dimensions, indexing works separately for each dimension separated by commas:\n",
        "arr2d = np.random.randint(0,100,(15,25)) #  create a 15x25 array of random ints\n",
        "print(\"row 5, column 9:\", arr2d[5,9])\n",
        "print(\"rows 2 to 4 and column 9:\", arr2d[2:4,9])\n",
        "print(\"rows 2 to 6 and columns 9 to 14:\", arr2d[2:6,9:14])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa3Z3kpd_ZKy"
      },
      "source": [
        "# We can also index using lists of indices:\n",
        "print(arr)\n",
        "# For 1 dimension\n",
        "print(\"positions 1, 4, and 7:\", arr[[1,4,7]])\n",
        "\n",
        "# For 2 (or more) dimensions\n",
        "print(\"positions (5,4), (2,8), (9,8):\", arr2d[(5,2,9), (4,8,8)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wJi9TP4ApqM"
      },
      "source": [
        "# We can also index using boolean arrays\n",
        "# get all values in arr greater than 75\n",
        "print(arr)\n",
        "print(arr>75)\n",
        "print(arr[arr>75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqM1AGHfBhNh"
      },
      "source": [
        "# Also works for multi dimensional arrays\n",
        "print(arr2d[arr2d > 75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bml-vAE8uY1"
      },
      "source": [
        "# We can use boolean indexing for assignment\n",
        "# Set all values in ar2d which are greater than 75 to 0\n",
        "arr2d[arr2d>75] = 0\n",
        "print(arr2d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7n8GC7ssYOZ"
      },
      "source": [
        "# Part 4.6 - LA-ICPMS data reduction excercise - *Workshop*\n",
        "\n",
        "In the geosciences, we often have raw measurement data from an analytical machine, and need to convert or \"reduce\" that data to make it useful for further analysis and interpretation. A very common example is conversion of mass spectrometer (or similar) raw count data to composition data, such as weight percentage or ppm of the analysis material. In many cases there are specific software packages to perform this data reduction without needing to do any coding, but frequently the underlying methodology is not complex and could be easily done in python. As an example, in this excercise we will convert raw Laser Ablation - ICP Mass Spectrometer (LA-ICPMS) data to mass fraction of the sample material.\n",
        "\n",
        "The following paper explains LA-ICPMS, typical data reduction proceedures and software packages:\n",
        "https://www.sciencedirect.com/science/article/abs/pii/S0009254118305461?via%3Dihub\n",
        "\n",
        "Using python, we will perform steps 2 to 5 of the \"Basic Processing\" in section 2.1 of that paper.\n",
        "\n",
        "**Note: Each field (within and beyond the geosciences) has its own literature about data reduction and processing. You should consult authoritative sources when doing this work to avoid methodological errors. This excercise is intended to demonstrate that the mathematical and programming required is easily achievable with only basic python knowledge.***\n",
        "\n",
        "**Data Reduction Steps**:\n",
        "* 1. Load the data\n",
        "* 2. Identification of background, samples and standards in the raw data\n",
        "* 3. Apply background correction\n",
        "* 4. Standardise data\n",
        "* 5. Calibrate data to standards\n",
        "* 6. Calculate the mass fraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWF3i3qLh9l9"
      },
      "source": [
        "## 4.6.1 - Load data\n",
        "\n",
        "The example data we will use is from the testing datasets for a python tool for LA-ICPMS data reduction:\n",
        "(https://github.com/oscarbranson/latools)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r42R3M1-q1Nv"
      },
      "source": [
        "# Load the data from here as a pandas data frame:\n",
        "# https://raw.githubusercontent.com/ds4geo/ds4geo/master/data/timeseries/laicpms_sample.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbE_pP54tKFg"
      },
      "source": [
        "# Create:\n",
        "# 1. a 1d numpy array of the \"Time\" column\n",
        "# 2. a 2d numpy array of the element count columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MTE9v1qcqgA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fGsi-3Cutsk"
      },
      "source": [
        "# Make a/some plots of the data to get an overview"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol1dkIRfdpcy"
      },
      "source": [
        "## 4.6.2 - Identify background, samples and standards\n",
        "\n",
        "When you plot the data, you will see several periods where the counts (for any element) are well above 0. The first 3 of these sections are standards, the last 4 are samples. The intermediate parts are background.\n",
        "\n",
        "We need to identify the time intervals corresponding to samples, sections and background for the following analyses. We will do this by identifying the start and end times/positions (here, position is measured in time) for each relevant section. We then create a boolean index array for each.\n",
        "\n",
        "It is recommended to work together with your classmates to complete this task by sharing the start and end positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J80xY262vcRJ"
      },
      "source": [
        "# Find out and record the start and end positions of at least 2 background sections\n",
        "# Record them in a list containing dictionaries with the following format:\n",
        "# [{\"start\": <position in seconds>, \"end\": <position in seconds>}\n",
        "#  {\"start\": <>, \"end\": <>},\n",
        "#   .....]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfdFI4eVxHWJ"
      },
      "source": [
        "# Do the same for all the sample sections, and separately for all the standards sections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBtZ16KdsX20"
      },
      "source": [
        "# Create a boolean index array for each of background, samples and standards.\n",
        "# Each should be same shape as the time array (use the .shape method to check).\n",
        "# hint: use np.logical_and to create a boolean index for each section,\n",
        "#       then combine the results with np.any. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfnmM_AKdIo8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0wlqUAYxsGP"
      },
      "source": [
        "## 4.6.3 - Apply background correction\n",
        "The background counts should be removed from the rest of the signal for each element.\n",
        "\n",
        "We therefore take the average counts during the background periods for each element and subtract these values from the element arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYRHa3cjzD5F"
      },
      "source": [
        "# Create an array of the average (mean) counts for each element during the background sections.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAM3WonJzMs_"
      },
      "source": [
        "# Subtract the per-element backgrounds from the entire dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLhnTepK4NW5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUTb69tF4WUj"
      },
      "source": [
        "##4.6.4 - Standardize data\n",
        "In LA-ICPMS analysis, the amount of analyte which is measured depends on how much is ablated by the laser, which in turn depends on the material properties of the sample (known as matrix effects). To remove matrix effects and other spurious features, we can standardize the data to an element which we expect to be present at a constant concentration. For carbonates, an isotope of Ca is often used and is known as the internal standard.\n",
        "\n",
        "Standardization in this case means that we convert all other element data to count ratios of that element vs Ca44."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boYpKYru5nL_"
      },
      "source": [
        "# Convert all data into ratios to Ca44 - i.e. divide all element counts by Ca44 counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhD4bRcl1WtV"
      },
      "source": [
        "##4.6.5 - Calibrate data\n",
        "Next we calibrate the count ratios to composition ratios. We do this using the measured standards of known composition.\n",
        "\n",
        "Preparation of the standard composition in terms of count ratio is beyond the scope of this excercise, so the required data is provided ready-to-use. In this case, we will only use one of the standards, but more complex methods exist to simultaneously calibrate using multiple standards and to thereby improve estimation of the measurement uncertainty.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8UR7Qk_289r"
      },
      "source": [
        "# Load the standard composition from here:\n",
        "# https://raw.githubusercontent.com/ds4geo/ds4geo/master/data/timeseries/laicpms_standard.csv\n",
        "# Convert the dataframe to a numpy array and check its shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byAhAfzA1ZLg"
      },
      "source": [
        "# The calibration data refers only to the first measured standard.\n",
        "# Create an boolean index array for this standard.\n",
        "# Then calculate the mean values per element for the standard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnukHuAu2Siy"
      },
      "source": [
        "# Calculate the conversion ratio by dividing calibration data by the measured (standardized) standard data\n",
        "# Apply the conversion ratio by dividing the standardized element data by the calibration\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR5cmfw5AKjI"
      },
      "source": [
        "# Remove all data except the samples\n",
        "# Assign everything else to np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-znfd4nSDIKc"
      },
      "source": [
        "##4.6.6 - Convert to mass fraction\n",
        "LA-ICPMS results are often reported simply as molar mass ratios to the internal standard (e.g. Ca), but one can also take an additional step to convert the molar mass ratios to the sample mass fraction.\n",
        "We standardized our data to an internal standard of constant concentration (i.e. a ratio of Ca44). However, if we want to calculate the mass fraction, we need to convert our data back from that ratio to the mass fraction. To do this, we need to know the concentration of the internal standard in the material. This needs to be separately measured or assumed.\n",
        "\n",
        "Here we will make an assumption and the calculations are provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7cZs4RMEp19"
      },
      "source": [
        "# Estimate mass fraction of Ca44 in sample (a foram)\n",
        "# Assume the foram is entirely composed of CaCO3\n",
        "# 1. Calculate the weight fraction of Ca in CaCO3\n",
        "Ca_aw = 40.078 # Calcium standard atomic weight\n",
        "O_aw = 15.999\n",
        "C_aw = 12.011\n",
        "CaCO3_aw = Ca_aw + C_aw + (3 * O_aw) # calculate atomic weight of CaCO3\n",
        "Ca_rw = Ca_aw / CaCO3_aw # Calculate Ca weight fraction of CaCO3\n",
        "\n",
        "# Assume the typical isotope fraction of Ca44 to all Ca\n",
        "# 2. Find the Ca44 weight fraction of CaCO3\n",
        "Ca44_ra = 0.02086 # relative abundance of Ca44 - i.e. 2%\n",
        "Ca44_rw = Ca_rw * Ca44_ra\n",
        "\n",
        "# 3. Convert to ppm\n",
        "Ca44_ppm = Ca44_rw * 1000000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9oZaWGh4Utm"
      },
      "source": [
        "# Apply the ratio to mass conversion by multiplying the calibrated ratio data by the Ca44 mass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Efr6klNAejq"
      },
      "source": [
        "# Make some plots to visualise the output.\n",
        "# Optionally you can convert the calibrated data back to a pandas DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqcuAsj6CLwl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scVek6Ncdynf"
      },
      "source": [
        "## 4.6.7 - Apply your code to another sample - *Challenge*\n",
        "The following data is from a copper age goat or sheep's tooth found at an Archaeological site in South Tirol. Its age is similar to that of the Otzi Ice mummy. One of the hypotheses about why Otzi was high in the mountains is that he was a shepherd. It is not clear whether high alpine \"Almwirtschaft\" style farming was practiced in Otzi's time, so this tooth, along with others from the same site were analysed for evidence of seasonal migration which could suggest Almwirtschaft was practiced at the time. Sheep, goat and cow 3rd molars grow throughout the first year of the animal's life and grow in layers. Laser ablation can be used to analyse the chemistry of these layers and thus a chemical timeline over ca. 1 year. As the chemistry of the tooth enamel depends on the animal's diet, and trace elements in the diet are affected by local geology, the hypothesis is that migration between different geological areas may be visible in these tooth trace element time-profiles.\n",
        "\n",
        "The data is from a single goat or sheep 3rd molar, representing ca. 1 year. The laser ablation trace (and major element) chemistry raw LA-ICPMS data can be found here (standards before and after, sample in the middle):\n",
        "https://github.com/ds4geo/ds4geo/blob/master/data/timeseries/la_icpms_486_487.csv\n",
        "\n",
        "Raw data here: https://raw.githubusercontent.com/ds4geo/ds4geo/master/data/timeseries/la_icpms_486_487.csv\n",
        "\n",
        "Apply your data reduction code to this dataset and visualise the resulting data. Do you see any interesting patterns.\n",
        " \n",
        "**note:** I don't know which standards were measured, but at least one of them is almost certainly one of the three which are in the data from the main workshop above. You can compare the element ratios from the 2 standards in this data to the 3 in the previous dataset and see if you can identify which is which. Otherwise just assume the first standard in the tooth dataset is the same as the one in the workshop dataset.\n"
      ]
    }
  ]
}